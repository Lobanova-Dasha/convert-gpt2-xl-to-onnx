{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convert GPT2-XL Transformer to ONXX model.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1Nxj0x9xXb52QKn43UKcmSZMSPplVk1Ol",
      "authorship_tag": "ABX9TyMgXNilidTyRArtvpj/zJSx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rayhern/convert-gpt2-xl-to-onnx/blob/master/Convert_GPT2_XL_Transformer_to_ONXX_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMeP6tQReZfa",
        "colab_type": "text"
      },
      "source": [
        "# Create ONNX model from transformers models. (Updated: 9-17-2020)\n",
        "- You must run this notebook in Google Colab Pro.\n",
        "- The instance needs to be of type GPU with High-RAM.\n",
        "- By: Ray Hernandez [github: @rayhern, twitter:@bizong]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2C3fXskxFMR",
        "colab_type": "text"
      },
      "source": [
        "# Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrU3lbPxtAS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBLWIJ00krza",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Install dependencies.\n",
        "!pip install onnx onnxruntime-gpu onnxruntime-tools transformers\n",
        "!pip install -U torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqTvoGcOQRmB",
        "colab_type": "text"
      },
      "source": [
        "#Convert GPT2-XL using ONNX Gpt2Helper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeDaHy3S3oSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "import os\n",
        "if os.path.exists('gpt2-final') is False:\n",
        "  print('making directory gpt2-final...')\n",
        "  %mkdir gpt2-final\n",
        "from onnxruntime_tools.transformers.gpt2_helper import Gpt2Helper, MyGPT2LMHeadModel\n",
        "# Can be encoded with cpu or gpu.\n",
        "device = 'cpu'\n",
        "# Use GPT2 model wrapper from onnxruntime's gpt2 helper supple.\n",
        "# conversion will not work without this wrapper.\n",
        "print('Downloading GPT2-XL...')\n",
        "model = MyGPT2LMHeadModel.from_pretrained('gpt2-xl')\n",
        "onnx_model_path = \"gpt2-final/gpt2-xl.onnx\"\n",
        "print('Converting model...')\n",
        "# Use GPT2Helper from onnxruntime tools to export GPT2-XL.\n",
        "Gpt2Helper.export_onnx(\n",
        "  model, \n",
        "  device, \n",
        "  onnx_model_path, \n",
        "  use_external_data_format=True, \n",
        "  verbose=True\n",
        ")\n",
        "print('Finished.')\n",
        "\n",
        "#/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py:712: \n",
        "# FutureWarning: The `past` argument is deprecated and will be removed in a \n",
        "# future version, use `past_key_values` instead. FutureWarning,\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap4WWjvIjgMy",
        "colab_type": "text"
      },
      "source": [
        "# Restart runtime to reclaim memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tdIm3mDjpTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ty2FqJpxhvS",
        "colab_type": "text"
      },
      "source": [
        "#Optimize GPT2-XL model and convert to float16."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxhrCqlPHl6f",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "from onnxruntime_tools import optimizer\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM\n",
        ")\n",
        "tf_model = AutoModelForCausalLM.from_pretrained('gpt2-xl')\n",
        "print('num heads: %s. hidden size: %s.' % (tf_model.config.n_head, tf_model.config.n_embd))\n",
        "optimized_model = optimizer.optimize_model(\n",
        "    \"gpt2-final/gpt2-xl.onnx\",\n",
        "    model_type='gpt2',\n",
        "    num_heads=tf_model.config.n_head,\n",
        "    hidden_size=tf_model.config.n_embd,\n",
        ")\n",
        "optimized_model.convert_model_float32_to_float16()\n",
        "optimized_model.change_input_to_int32()\n",
        "optimized_model.save_model_to_file(\"gpt2-final/gpt2_fp16.onnx\", use_external_data_format=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7849x3H6xx5",
        "colab_type": "text"
      },
      "source": [
        "#ONNX Model Text Generation Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXgFV3scD2OI",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Define GPT2 ONNX Text Generation Class\n",
        "\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "from psutil import cpu_count\n",
        "from os import environ\n",
        "# Constants from the performance optimization available in onnxruntime\n",
        "# It needs to be done before importing onnxruntime\n",
        "environ[\"OMP_NUM_THREADS\"] = str(cpu_count(logical=True))\n",
        "environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'\n",
        "from onnxruntime_tools.transformers.gpt2_helper import Gpt2Helper\n",
        "from onnxruntime.capi._pybind_state import set_seed as ort_set_seed\n",
        "import onnxruntime as ort\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "import numpy\n",
        "from typing import Iterable, List, Optional, Tuple\n",
        "import random\n",
        "\n",
        "@torch.no_grad()\n",
        "def calc_banned_ngram_tokens(prev_input_ids: Tensor, num_hypos: int, no_repeat_ngram_size: int, cur_len: int) -> List:\n",
        "    \"\"\"Copied from fairseq for no_repeat_ngram in beam_search\"\"\"\n",
        "    if cur_len + 1 < no_repeat_ngram_size:\n",
        "        # return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\n",
        "        return [[] for _ in range(num_hypos)]\n",
        "    generated_ngrams = [{} for _ in range(num_hypos)]\n",
        "    for idx in range(num_hypos):\n",
        "        gen_tokens = prev_input_ids[idx].tolist()\n",
        "        generated_ngram = generated_ngrams[idx]\n",
        "        for ngram in zip(*[gen_tokens[i:] for i in range(no_repeat_ngram_size)]):\n",
        "            prev_ngram_tuple = tuple(ngram[:-1])\n",
        "            generated_ngram[prev_ngram_tuple] = generated_ngram.get(prev_ngram_tuple, []) + [ngram[-1]]\n",
        "\n",
        "    def _get_generated_ngrams(hypo_idx):\n",
        "        # Before decoding the next token, prevent decoding of ngrams that have already appeared\n",
        "        start_idx = cur_len + 1 - no_repeat_ngram_size\n",
        "        ngram_idx = tuple(prev_input_ids[hypo_idx, start_idx:cur_len].tolist())\n",
        "        return generated_ngrams[hypo_idx].get(ngram_idx, [])\n",
        "\n",
        "    banned_tokens = [_get_generated_ngrams(hypo_idx) for hypo_idx in range(num_hypos)]\n",
        "    return banned_tokens\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def enforce_repetition_penalty_(lprobs, batch_size, num_beams, prev_output_tokens, repetition_penalty):\n",
        "    \"\"\"\n",
        "    Enforce the repetition penalty (from the `CTRL paper <https://arxiv.org/abs/1909.05858>`__).\n",
        "    \"\"\"\n",
        "    for i in range(batch_size * num_beams):\n",
        "        for previous_token in set(prev_output_tokens[i].tolist()):\n",
        "            # if score < 0 then repetition penalty has to multiplied to reduce the previous token probability\n",
        "            if lprobs[i, previous_token] < 0:\n",
        "                lprobs[i, previous_token] *= repetition_penalty\n",
        "            else:\n",
        "                lprobs[i, previous_token] /= repetition_penalty\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def calc_banned_bad_words_ids(prev_input_ids: Iterable[int], bad_words_ids: Iterable[int]) -> Iterable[int]:\n",
        "    banned_tokens = []\n",
        "    def _tokens_match(prev_tokens, tokens):\n",
        "        if len(tokens) == 0:\n",
        "            # if bad word tokens is just one token always ban it\n",
        "            return True\n",
        "        if len(tokens) > len(prev_tokens):\n",
        "            # if bad word tokens are longer than prev tokens they can't be equal\n",
        "            return False\n",
        "\n",
        "        if prev_tokens[-len(tokens):] == tokens:\n",
        "            # if tokens match\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    for prev_input_ids_slice in prev_input_ids:\n",
        "        banned_tokens_slice = []\n",
        "        for banned_token_seq in bad_words_ids:\n",
        "            assert len(banned_token_seq) > 0, \"Banned words token sequences {} cannot have an empty list\".format(\n",
        "                bad_words_ids\n",
        "            )\n",
        "            if _tokens_match(prev_input_ids_slice, banned_token_seq[:-1]) is False:\n",
        "                # if tokens do not match continue\n",
        "                continue\n",
        "            banned_tokens_slice.append(banned_token_seq[-1])\n",
        "        banned_tokens.append(banned_tokens_slice)\n",
        "    return banned_tokens\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def set_scores_to_inf_for_banned_tokens(scores: torch.Tensor, banned_tokens: List[List[int]]) -> None:\n",
        "    \"\"\"Modifies the scores in place by setting the banned token positions to `-inf`. Banned token is expected to be\n",
        "    a list of list of banned tokens to ban in the format [[batch index, vocabulary position],...]\n",
        "        Args:\n",
        "            scores: logits distribution of shape (batch size, vocabulary size)\n",
        "            banned_tokens: list of list of tokens to ban of length (batch_size)\n",
        "    \"\"\"\n",
        "    banned_mask_list = []\n",
        "    for idx, batch_banned_tokens in enumerate(banned_tokens):\n",
        "        for token in batch_banned_tokens:\n",
        "            banned_mask_list.append([idx, token])\n",
        "    if not banned_mask_list:\n",
        "        return\n",
        "    banned_mask = torch.LongTensor(banned_mask_list)\n",
        "    indices = torch.ones(len(banned_mask))\n",
        "    # A sparse tensor is generated from a list of coordinates: [[0, 1], [0, 2], [2, 0]].\n",
        "    banned_mask = torch.sparse.Tensor(banned_mask.t(), indices, scores.size()).to(scores.device).to_dense().bool()\n",
        "    scores.masked_fill_(banned_mask, -float(\"inf\"))\n",
        "\n",
        "\n",
        "class BeamHypotheses(object):\n",
        "    def __init__(self, num_beams, max_length, length_penalty, early_stopping):\n",
        "        \"\"\"\n",
        "        Initialize n-best list of hypotheses.\n",
        "        \"\"\"\n",
        "        self.max_length = max_length - 1  # ignoring bos_token\n",
        "        self.length_penalty = length_penalty\n",
        "        self.early_stopping = early_stopping\n",
        "        self.num_beams = num_beams\n",
        "        self.beams = []\n",
        "        self.worst_score = 1e9\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Number of hypotheses in the list.\n",
        "        \"\"\"\n",
        "        return len(self.beams)\n",
        "\n",
        "    def add(self, hyp, sum_logprobs):\n",
        "        \"\"\"\n",
        "        Add a new hypothesis to the list.\n",
        "        \"\"\"\n",
        "        score = sum_logprobs / len(hyp) ** self.length_penalty\n",
        "        if len(self) < self.num_beams or score > self.worst_score:\n",
        "            self.beams.append((score, hyp))\n",
        "            if len(self) > self.num_beams:\n",
        "                sorted_scores = sorted([(s, idx) for idx, (s, _) in enumerate(self.beams)])\n",
        "                del self.beams[sorted_scores[0][1]]\n",
        "                self.worst_score = sorted_scores[1][0]\n",
        "            else:\n",
        "                self.worst_score = min(score, self.worst_score)\n",
        "\n",
        "    def is_done(self, best_sum_logprobs, cur_len):\n",
        "        \"\"\"\n",
        "        If there are enough hypotheses and that none of the hypotheses being generated\n",
        "        can become better than the worst one in the heap, then we are done with this sentence.\n",
        "        \"\"\"\n",
        "\n",
        "        if len(self) < self.num_beams:\n",
        "            return False\n",
        "        elif self.early_stopping:\n",
        "            return True\n",
        "        else:\n",
        "            cur_score = best_sum_logprobs / cur_len ** self.length_penalty\n",
        "            ret = self.worst_score >= cur_score\n",
        "            return ret\n",
        "\n",
        "\n",
        "class GPT2ONNXModel:\n",
        "    def __init__(self, onnx_model_path, gpt2_model_path, device='cuda', verbose=False, threads=None, is_float16=False):\n",
        "        self.config = AutoConfig.from_pretrained(gpt2_model_path)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(gpt2_model_path)\n",
        "        self.tokenizer.padding_side = \"left\"\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.num_attention_heads = self.config.n_head\n",
        "        self.hidden_size = self.config.n_embd\n",
        "        self.num_layer = self.config.n_layer\n",
        "        self.verbose = verbose\n",
        "        self.is_float16 = is_float16\n",
        "        # Set the seed for onnxruntime.\n",
        "        torch.random.manual_seed(random.randint(1, 9999999))\n",
        "        torch.cuda.manual_seed_all(random.randint(1, 9999999))\n",
        "        torch.manual_seed(random.randint(1, 9999999))\n",
        "        numpy.random.seed(random.randint(1, 9999999))\n",
        "        ort.set_seed(random.randint(1, 9999999))\n",
        "        # Set our session options.\n",
        "        options = ort.SessionOptions()\n",
        "        # set session threads if user supplied.\n",
        "        if threads is not None:\n",
        "            options.inter_op_num_threads = threads\n",
        "            options.intra_op_num_threads = threads\n",
        "        # options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
        "        # Only CPU supports parallel.\n",
        "        if device == 'cpu':\n",
        "            options.execution_mode = ort.ExecutionMode.ORT_PARALLEL\n",
        "        # Start the ONNX session.\n",
        "        self.session = ort.InferenceSession(\n",
        "            onnx_model_path,\n",
        "            sess_options=options,\n",
        "            providers=['CUDAExecutionProvider' if device == 'cuda' else 'CPUExecutionProvider']\n",
        "        )\n",
        "        self.device = device\n",
        "        if self.verbose is True:\n",
        "            print('ONNX Session Created!')\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, input_text, max_length: Optional[int] = None, min_length: Optional[int] = None, do_sample: Optional[bool] = None,\n",
        "        early_stopping: Optional[bool] = None, num_beams: Optional[int] = None, temperature: Optional[float] = None,\n",
        "        top_k: Optional[int] = None, top_p: Optional[float] = None, repetition_penalty: Optional[float] = None, bad_words_ids: Optional[Iterable[int]] = None,\n",
        "        bos_token_id: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Optional[int] = None, length_penalty: Optional[float] = None,\n",
        "        no_repeat_ngram_size: Optional[int] = None, num_return_sequences: Optional[int] = None, attention_mask: Optional[torch.LongTensor] = None,\n",
        "        decoder_start_token_id: Optional[int] = None, use_cache: Optional[bool] = None,\n",
        "        **model_kwargs\n",
        "    ) -> torch.LongTensor:\n",
        "        r\"\"\"\n",
        "        Generates sequences for models with a language modeling head. The method currently supports greedy decoding,\n",
        "        beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.\n",
        "        \"\"\"\n",
        "        # Get inputs from input text. (input ids, attention mask, position, and past)\n",
        "        input_ids, attention_mask, position_ids, past = self.get_inputs(input_text)\n",
        "        # Get user supplied text generation options. Replace with model config on unset options.\n",
        "        max_length = max_length if max_length is not None else self.config.max_length\n",
        "        min_length = min_length if min_length is not None else self.config.min_length\n",
        "        do_sample = do_sample if do_sample is not None else self.config.do_sample\n",
        "        early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        num_beams = num_beams if num_beams is not None else self.config.num_beams\n",
        "        temperature = temperature if temperature is not None else self.config.temperature\n",
        "        top_k = top_k if top_k is not None else self.config.top_k\n",
        "        top_p = top_p if top_p is not None else self.config.top_p\n",
        "        repetition_penalty = repetition_penalty if repetition_penalty is not None else self.config.repetition_penalty\n",
        "        bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id\n",
        "        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
        "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
        "        length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n",
        "        no_repeat_ngram_size = (\n",
        "            no_repeat_ngram_size if no_repeat_ngram_size is not None else self.config.no_repeat_ngram_size\n",
        "        )\n",
        "        bad_words_ids = bad_words_ids if bad_words_ids is not None else self.config.bad_words_ids\n",
        "        num_return_sequences = (\n",
        "            num_return_sequences if num_return_sequences is not None else self.config.num_return_sequences\n",
        "        )\n",
        "        decoder_start_token_id = (\n",
        "            decoder_start_token_id if decoder_start_token_id is not None else self.config.decoder_start_token_id\n",
        "        )\n",
        "        if input_ids is not None:\n",
        "            batch_size = input_ids.shape[0]  # overriden by the input batch_size\n",
        "        else:\n",
        "            batch_size = 1\n",
        "        # Make sure all options were set correctly.\n",
        "        assert isinstance(max_length, int) and max_length > 0, \"`max_length` should be a strictly positive integer.\"\n",
        "        assert isinstance(min_length, int) and min_length >= 0, \"`min_length` should be a positive integer.\"\n",
        "        assert isinstance(do_sample, bool), \"`do_sample` should be a boolean.\"\n",
        "        assert isinstance(early_stopping, bool), \"`early_stopping` should be a boolean.\"\n",
        "        assert isinstance(use_cache, bool), \"`use_cache` should be a boolean.\"\n",
        "        assert isinstance(num_beams, int) and num_beams > 0, \"`num_beams` should be a strictly positive integer.\"\n",
        "        assert temperature > 0, \"`temperature` should be strictly positive.\"\n",
        "        assert isinstance(top_k, int) and top_k >= 0, \"`top_k` should be a positive integer.\"\n",
        "        assert 0 <= top_p <= 1, \"`top_p` should be between 0 and 1.\"\n",
        "        assert repetition_penalty >= 1.0, \"`repetition_penalty` should be >= 1.\"\n",
        "        assert pad_token_id is None or (\n",
        "            isinstance(pad_token_id, int) and (pad_token_id >= 0)\n",
        "        ), \"`pad_token_id` should be a positive integer.\"\n",
        "        assert (eos_token_id is None) or (\n",
        "            isinstance(eos_token_id, int) and (eos_token_id >= 0)\n",
        "        ), \"`eos_token_id` should be a positive integer.\"\n",
        "        assert length_penalty > 0, \"`length_penalty` should be strictly positive.\"\n",
        "        assert (\n",
        "            isinstance(no_repeat_ngram_size, int) and no_repeat_ngram_size >= 0\n",
        "        ), \"`no_repeat_ngram_size` should be a positive integer.\"\n",
        "        assert (\n",
        "            isinstance(num_return_sequences, int) and num_return_sequences > 0\n",
        "        ), \"`num_return_sequences` should be a strictly positive integer.\"\n",
        "        assert (\n",
        "            bad_words_ids is None or isinstance(bad_words_ids, list) and isinstance(bad_words_ids[0], list)\n",
        "        ), \"`bad_words_ids` is either `None` or a list of lists of tokens that should not be generated\"\n",
        "\n",
        "        # not allow to duplicate outputs when greedy decoding\n",
        "        if do_sample is False:\n",
        "            if num_beams == 1:\n",
        "                # no_beam_search greedy generation conditions\n",
        "                assert (\n",
        "                    num_return_sequences == 1\n",
        "                ), \"Greedy decoding will always produce the same output for num_beams == 1 and num_return_sequences > 1. Please set num_return_sequences = 1\"\n",
        "\n",
        "            else:\n",
        "                # beam_search greedy generation conditions\n",
        "                assert (\n",
        "                    num_beams >= num_return_sequences\n",
        "                ), \"Greedy beam search decoding cannot return more sequences than it has beams. Please set num_beams >= num_return_sequences\"\n",
        "\n",
        "        max_length = len(input_ids[0]) + max_length\n",
        "\n",
        "        # create attention mask if necessary\n",
        "        # TODO (PVP): this should later be handled by the forward fn() in each model in the future see PR 3140\n",
        "        if (attention_mask is None) and (pad_token_id is not None) and (pad_token_id in input_ids):\n",
        "            attention_mask = input_ids.ne(pad_token_id).long()\n",
        "        elif attention_mask is None:\n",
        "            attention_mask = input_ids.new_ones(input_ids.shape)\n",
        "\n",
        "        # set pad_token_id to eos_token_id if not set. Important that this is done after\n",
        "        # attention_mask is created\n",
        "        if pad_token_id is None and eos_token_id is not None:\n",
        "            pad_token_id = eos_token_id\n",
        "\n",
        "        # current position and vocab size\n",
        "        if hasattr(self.config, \"vocab_size\"):\n",
        "            vocab_size = self.config.vocab_size\n",
        "        elif (\n",
        "            self.config.is_encoder_decoder\n",
        "            and hasattr(self.config, \"decoder\")\n",
        "            and hasattr(self.config.decoder, \"vocab_size\")\n",
        "        ):\n",
        "            vocab_size = self.config.decoder.vocab_size\n",
        "\n",
        "        # set effective batch size and effective batch multiplier according to do_sample\n",
        "        if do_sample:\n",
        "            effective_batch_size = batch_size * num_return_sequences\n",
        "            effective_batch_mult = num_return_sequences\n",
        "        else:\n",
        "            effective_batch_size = batch_size\n",
        "            effective_batch_mult = 1\n",
        "\n",
        "        if self.config.is_encoder_decoder:\n",
        "            if decoder_start_token_id is None:\n",
        "                # see if BOS token can be used for decoder_start_token_id\n",
        "                if bos_token_id is not None:\n",
        "                    decoder_start_token_id = bos_token_id\n",
        "                elif hasattr(self.config, \"decoder\") and hasattr(self.config.decoder, \"bos_token_id\"):\n",
        "                    decoder_start_token_id = self.config.decoder.bos_token_id\n",
        "                else:\n",
        "                    raise ValueError(\n",
        "                        \"decoder_start_token_id or bos_token_id has to be defined for encoder-decoder generation\"\n",
        "                    )\n",
        "\n",
        "            assert hasattr(self, \"get_encoder\"), \"{} should have a 'get_encoder' function defined\".format(self)\n",
        "            assert callable(self.get_encoder), \"{} should be a method\".format(self.get_encoder)\n",
        "\n",
        "            # get encoder and store encoder outputs\n",
        "            encoder = self.get_encoder()\n",
        "            encoder_outputs: ModelOutput = encoder(input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "\n",
        "        # Expand input ids if num_beams > 1 or num_return_sequences > 1\n",
        "        if num_return_sequences > 1 or num_beams > 1:\n",
        "            input_ids_len = input_ids.shape[-1]\n",
        "            input_ids = input_ids.unsqueeze(1).expand(batch_size, effective_batch_mult * num_beams, input_ids_len)\n",
        "            attention_mask = attention_mask.unsqueeze(1).expand(\n",
        "                batch_size, effective_batch_mult * num_beams, input_ids_len\n",
        "            )\n",
        "\n",
        "            input_ids = input_ids.contiguous().view(\n",
        "                effective_batch_size * num_beams, input_ids_len\n",
        "            )  # shape: (batch_size * num_return_sequences * num_beams, cur_len)\n",
        "            attention_mask = attention_mask.contiguous().view(\n",
        "                effective_batch_size * num_beams, input_ids_len\n",
        "            )  # shape: (batch_size * num_return_sequences * num_beams, cur_len)\n",
        "\n",
        "        if self.config.is_encoder_decoder:\n",
        "            # create empty decoder_input_ids\n",
        "            input_ids = torch.full(\n",
        "                (effective_batch_size * num_beams, 1),\n",
        "                decoder_start_token_id,\n",
        "                dtype=torch.long,\n",
        "                device=self.device,\n",
        "            )\n",
        "            cur_len = 1\n",
        "\n",
        "            assert (\n",
        "                batch_size == encoder_outputs.last_hidden_state.shape[0]\n",
        "            ), f\"expected encoder_outputs.last_hidden_state to have 1st dimension bs={batch_size}, got {encoder_outputs.last_hidden_state.shape[0]} \"\n",
        "\n",
        "            # expand batch_idx to assign correct encoder output for expanded input_ids (due to num_beams > 1 and num_return_sequences > 1)\n",
        "            expanded_batch_idxs = (\n",
        "                torch.arange(batch_size)\n",
        "                .view(-1, 1)\n",
        "                .repeat(1, num_beams * effective_batch_mult)\n",
        "                .view(-1)\n",
        "                .to(input_ids.device)\n",
        "            )\n",
        "\n",
        "            # expand encoder_outputs\n",
        "            encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.index_select(\n",
        "                0, expanded_batch_idxs\n",
        "            )\n",
        "\n",
        "            # save encoder_outputs in `model_kwargs`\n",
        "            model_kwargs[\"encoder_outputs\"] = encoder_outputs\n",
        "\n",
        "        else:\n",
        "            cur_len = input_ids.shape[-1]\n",
        "\n",
        "        assert (\n",
        "            cur_len < max_length\n",
        "        ), f\"The context has {cur_len} number of tokens, but `max_length` is only {max_length}. Please make sure that `max_length` is bigger than the number of tokens, by setting either `generate(max_length=...,...)` or `config.max_length = ...`\"\n",
        "\n",
        "        if num_beams > 1:\n",
        "            output = self._generate_beam_search(input_ids, cur_len=cur_len, max_length=max_length, min_length=min_length, do_sample=do_sample, early_stopping=early_stopping,\n",
        "                temperature=temperature, top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty, no_repeat_ngram_size=no_repeat_ngram_size, bad_words_ids=bad_words_ids,\n",
        "                pad_token_id=pad_token_id, eos_token_id=eos_token_id, batch_size=effective_batch_size, num_return_sequences=num_return_sequences, length_penalty=length_penalty,\n",
        "                num_beams=num_beams, vocab_size=vocab_size, attention_mask=attention_mask, past=past, position_ids=position_ids, use_cache=use_cache, model_kwargs=model_kwargs,\n",
        "            )\n",
        "        else:\n",
        "            output = self._generate_no_beam_search(\n",
        "                input_ids, cur_len=cur_len, max_length=max_length, min_length=min_length, do_sample=do_sample, temperature=temperature,\n",
        "                top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty, no_repeat_ngram_size=no_repeat_ngram_size, bad_words_ids=bad_words_ids,\n",
        "                pad_token_id=pad_token_id, eos_token_id=eos_token_id, batch_size=effective_batch_size, attention_mask=attention_mask,\n",
        "                use_cache=use_cache, model_kwargs=model_kwargs,\n",
        "            )\n",
        "\n",
        "        output = self.tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "        return output\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _generate_no_beam_search(self, input_ids, cur_len, max_length, min_length, do_sample, temperature,\n",
        "        top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, pad_token_id, eos_token_id,\n",
        "        batch_size, attention_mask, use_cache, model_kwargs,\n",
        "    ):\n",
        "        \"\"\"Generate sequences for each example without beam search (num_beams == 1).\n",
        "        All returned sequence are generated independantly.\n",
        "        \"\"\"\n",
        "        # length of generated sentences / unfinished sentences\n",
        "        unfinished_sents = input_ids.new(batch_size).fill_(1)\n",
        "        sent_lengths = input_ids.new(batch_size).fill_(max_length)\n",
        "\n",
        "        past_shape = [2, batch_size, self.num_attention_heads, 0, self.hidden_size // self.num_attention_heads]\n",
        "        past = []\n",
        "        for i in range(self.num_layer):\n",
        "            past.append(torch.empty(past_shape, dtype=torch.float32).to(self.device))\n",
        "        attention_mask = attention_mask.clone().detach()\n",
        "        \n",
        "        while cur_len < max_length:\n",
        "\n",
        "            model_inputs = self.prepare_inputs_for_generation(\n",
        "                input_ids, past=past, attention_mask=attention_mask, use_cache=use_cache, **model_kwargs\n",
        "            )\n",
        "\n",
        "            position_ids = (attention_mask.long().cumsum(-1) - 1).to(self.device)\n",
        "            position_ids.masked_fill_(position_ids < 0, 0)\n",
        "\n",
        "            outputs = self.inference_with_io_binding(\n",
        "                input_ids,\n",
        "                position_ids,\n",
        "                attention_mask,\n",
        "                past\n",
        "            )\n",
        "            next_token_logits = outputs[0][:, -1, :]\n",
        "\n",
        "            scores = self.postprocess_next_token_scores(scores=next_token_logits, input_ids=input_ids, no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "                bad_words_ids=bad_words_ids, cur_len=cur_len, min_length=min_length, max_length=max_length, eos_token_id=eos_token_id,\n",
        "                repetition_penalty=repetition_penalty, batch_size=batch_size, num_beams=1,\n",
        "            )\n",
        "\n",
        "            # if model has past, then set the past variable to speed up decoding\n",
        "            # if \"past_key_values\" in outputs:\n",
        "            #     past = outputs.past_key_values\n",
        "            # elif \"mems\" in outputs:\n",
        "            #     past = outputs.mems\n",
        "\n",
        "            if do_sample:\n",
        "                # Temperature (higher temperature => more likely to sample low probability tokens)\n",
        "                if temperature != 1.0:\n",
        "                    scores = scores / temperature\n",
        "                # Top-p/top-k filtering\n",
        "                next_token_logscores = self.top_k_top_p_filtering(scores, top_k=top_k, top_p=top_p)\n",
        "                # Sample\n",
        "                probs = F.softmax(next_token_logscores, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
        "            else:\n",
        "                # Greedy decoding\n",
        "                next_token = torch.argmax(next_token_logits, dim=-1)\n",
        "\n",
        "            # update generations and finished sentences\n",
        "            if eos_token_id is not None:\n",
        "                # pad finished sentences if eos_token_id exist\n",
        "                tokens_to_add = next_token * unfinished_sents + (pad_token_id) * (1 - unfinished_sents)\n",
        "            else:\n",
        "                tokens_to_add = next_token\n",
        "\n",
        "            # add token and increase length by one\n",
        "            input_ids = torch.cat([input_ids, tokens_to_add.unsqueeze(-1)], dim=-1)\n",
        "            cur_len = cur_len + 1\n",
        "\n",
        "            if eos_token_id is not None:\n",
        "                eos_in_sents = tokens_to_add == eos_token_id\n",
        "                # if sentence is unfinished and the token to add is eos, sent_lengths is filled with current length\n",
        "                is_sents_unfinished_and_token_to_add_is_eos = unfinished_sents.mul(eos_in_sents.long()).bool()\n",
        "                sent_lengths.masked_fill_(is_sents_unfinished_and_token_to_add_is_eos, cur_len)\n",
        "                # unfinished_sents is set to zero if eos in sentence\n",
        "                unfinished_sents.mul_((~eos_in_sents).long())\n",
        "\n",
        "            # stop when there is a </s> in each sentence, or if we exceed the maximul length\n",
        "            if unfinished_sents.max() == 0:\n",
        "                break\n",
        "\n",
        "            # extend attention_mask for new generated input if only decoder\n",
        "            if self.config.is_encoder_decoder is False:\n",
        "                attention_mask = torch.cat(\n",
        "                    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n",
        "                )\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _generate_beam_search(self, input_ids, cur_len, max_length, min_length, do_sample, early_stopping, temperature,\n",
        "        top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, pad_token_id, eos_token_id, batch_size,\n",
        "        num_return_sequences, length_penalty, num_beams, vocab_size, attention_mask, past, position_ids, use_cache,\n",
        "        model_kwargs,\n",
        "    ):\n",
        "        \"\"\"Generate sequences for each example with beam search.\"\"\"\n",
        "\n",
        "        # generated hypotheses\n",
        "        generated_hyps = [\n",
        "            BeamHypotheses(num_beams, max_length, length_penalty, early_stopping=early_stopping)\n",
        "            for _ in range(batch_size)\n",
        "        ]\n",
        "\n",
        "        # scores for each sentence in the beam\n",
        "        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n",
        "\n",
        "        # for greedy decoding it is made sure that only tokens of the first beam are considered to avoid sampling the exact same tokens three times\n",
        "        if do_sample is False:\n",
        "            beam_scores[:, 1:] = -1e9\n",
        "        beam_scores = beam_scores.view(-1)  # shape (batch_size * num_beams,)\n",
        "\n",
        "        # done sentences\n",
        "        done = [False for _ in range(batch_size)]\n",
        "\n",
        "        past_shape = [2, batch_size * num_beams, self.num_attention_heads, 0, self.hidden_size // self.num_attention_heads]\n",
        "        past = []\n",
        "        for i in range(self.num_layer):\n",
        "            past.append(torch.empty(past_shape, dtype=torch.float32).to(self.device))\n",
        "\n",
        "        while cur_len < max_length:\n",
        "            model_inputs = self.prepare_inputs_for_generation(\n",
        "                input_ids, past=past, attention_mask=attention_mask, use_cache=use_cache, **model_kwargs\n",
        "            )\n",
        "            position_ids = (attention_mask.long().cumsum(-1) - 1).to(self.device)\n",
        "            position_ids.masked_fill_(position_ids < 0, 0)\n",
        "            # outputs = self(**model_inputs, return_dict=True)  # (batch_size * num_beams, cur_len, vocab_size)\n",
        "            outputs = self.inference_with_io_binding(\n",
        "                input_ids,\n",
        "                position_ids,\n",
        "                attention_mask,\n",
        "                past\n",
        "            )\n",
        "            next_token_logits = outputs[0][:, -1, :]  # (batch_size * num_beams, vocab_size)\n",
        "\n",
        "            # if model has past, then set the past variable to speed up decoding\n",
        "            # if \"past_key_values\" in outputs:\n",
        "            #     past = outputs.past_key_values\n",
        "            # elif \"mems\" in outputs:\n",
        "            #     past = outputs.mems\n",
        "\n",
        "            if self.config.is_encoder_decoder and do_sample is False:\n",
        "                # TODO (PVP) still a bit hacky here - there might be a better solution\n",
        "                next_token_logits = self.adjust_logits_during_generation(\n",
        "                    next_token_logits, cur_len=cur_len, max_length=max_length\n",
        "                )\n",
        "\n",
        "            scores = F.log_softmax(next_token_logits, dim=-1)  # (batch_size * num_beams, vocab_size)\n",
        "\n",
        "            scores = self.postprocess_next_token_scores(scores=scores, input_ids=input_ids, no_repeat_ngram_size=no_repeat_ngram_size, bad_words_ids=bad_words_ids,\n",
        "                cur_len=cur_len, min_length=min_length, max_length=max_length, eos_token_id=eos_token_id, repetition_penalty=repetition_penalty,\n",
        "                batch_size=batch_size, num_beams=num_beams,\n",
        "            )\n",
        "\n",
        "            assert scores.shape == (batch_size * num_beams, vocab_size), \"Shapes of scores: {} != {}\".format(\n",
        "                scores.shape, (batch_size * num_beams, vocab_size)\n",
        "            )\n",
        "\n",
        "            if do_sample:\n",
        "                _scores = scores + beam_scores[:, None].expand_as(scores)  # (batch_size * num_beams, vocab_size)\n",
        "                # Temperature\n",
        "                if temperature != 1.0:\n",
        "                    _scores = _scores / temperature\n",
        "                # Top-p/top-k filtering\n",
        "                _scores = self.top_k_top_p_filtering(\n",
        "                    _scores, top_k=top_k, top_p=top_p, min_tokens_to_keep=2\n",
        "                )  # (batch_size * num_beams, vocab_size)\n",
        "                # re-organize to group the beam together to sample from all beam_idxs\n",
        "                _scores = _scores.contiguous().view(\n",
        "                    batch_size, num_beams * vocab_size\n",
        "                )  # (batch_size, num_beams * vocab_size)\n",
        "\n",
        "                # Sample 2 next tokens for each beam (so we have some spare tokens and match output of greedy beam search)\n",
        "                probs = F.softmax(_scores, dim=-1)\n",
        "                next_tokens = torch.multinomial(probs, num_samples=2 * num_beams)  # (batch_size, num_beams * 2)\n",
        "                # Compute next scores\n",
        "                next_scores = torch.gather(_scores, -1, next_tokens)  # (batch_size, num_beams * 2)\n",
        "                # sort the sampled vector to make sure that the first num_beams samples are the best\n",
        "                next_scores, next_scores_indices = torch.sort(next_scores, descending=True, dim=1)\n",
        "                next_tokens = torch.gather(next_tokens, -1, next_scores_indices)  # (batch_size, num_beams * 2)\n",
        "\n",
        "            else:\n",
        "                next_scores = scores + beam_scores[:, None].expand_as(scores)  # (batch_size * num_beams, vocab_size)\n",
        "\n",
        "                # re-organize to group the beam together (we are keeping top hypothesis accross beams)\n",
        "                next_scores = next_scores.view(\n",
        "                    batch_size, num_beams * vocab_size\n",
        "                )  # (batch_size, num_beams * vocab_size)\n",
        "\n",
        "                next_scores, next_tokens = torch.topk(next_scores, 2 * num_beams, dim=1, largest=True, sorted=True)\n",
        "\n",
        "            assert next_scores.size() == next_tokens.size() == (batch_size, 2 * num_beams)\n",
        "\n",
        "            # next batch beam content\n",
        "            next_batch_beam = []\n",
        "\n",
        "            # for each sentence\n",
        "            for batch_idx in range(batch_size):\n",
        "\n",
        "                # if we are done with this sentence, add a pad token\n",
        "                if done[batch_idx]:\n",
        "                    assert (\n",
        "                        len(generated_hyps[batch_idx]) >= num_beams\n",
        "                    ), \"Batch can only be done if at least {} beams have been generated\".format(num_beams)\n",
        "                    assert (\n",
        "                        eos_token_id is not None and pad_token_id is not None\n",
        "                    ), \"generated beams >= num_beams -> eos_token_id and pad_token have to be defined\"\n",
        "                    next_batch_beam.extend([(0, pad_token_id, 0)] * num_beams)  # pad the batch\n",
        "                    continue\n",
        "\n",
        "                # next sentence beam content, this will get added to next_batch_beam\n",
        "                next_sent_beam = []\n",
        "\n",
        "                # next tokens for this sentence\n",
        "                for beam_token_rank, (beam_token_id, beam_token_score) in enumerate(\n",
        "                    zip(next_tokens[batch_idx], next_scores[batch_idx])\n",
        "                ):\n",
        "                    # get beam and token IDs\n",
        "                    beam_id = beam_token_id // vocab_size\n",
        "                    token_id = beam_token_id % vocab_size\n",
        "\n",
        "                    effective_beam_id = batch_idx * num_beams + beam_id\n",
        "                    # add to generated hypotheses if end of sentence\n",
        "                    if (eos_token_id is not None) and (token_id.item() == eos_token_id):\n",
        "                        # if beam_token does not belong to top num_beams tokens, it should not be added\n",
        "                        is_beam_token_worse_than_top_num_beams = beam_token_rank >= num_beams\n",
        "                        if is_beam_token_worse_than_top_num_beams:\n",
        "                            continue\n",
        "                        generated_hyps[batch_idx].add(\n",
        "                            input_ids[effective_beam_id].clone(),\n",
        "                            beam_token_score.item(),\n",
        "                        )\n",
        "                    else:\n",
        "                        # add next predicted token since it is not eos_token\n",
        "                        next_sent_beam.append((beam_token_score, token_id, effective_beam_id))\n",
        "\n",
        "                    # once the beam for next step is full, don't add more tokens to it.\n",
        "                    if len(next_sent_beam) == num_beams:\n",
        "                        break\n",
        "\n",
        "                # Check if we are done so that we can save a pad step if all(done)\n",
        "                done[batch_idx] = done[batch_idx] or generated_hyps[batch_idx].is_done(\n",
        "                    next_scores[batch_idx].max().item(), cur_len\n",
        "                )\n",
        "\n",
        "                # update next beam content\n",
        "                assert len(next_sent_beam) == num_beams, \"Beam should always be full\"\n",
        "                next_batch_beam.extend(next_sent_beam)\n",
        "                assert len(next_batch_beam) == num_beams * (batch_idx + 1), \"We should have added num_beams each step\"\n",
        "\n",
        "            # stop when we are done with each sentence\n",
        "            if all(done):\n",
        "                break\n",
        "\n",
        "            # sanity check / prepare next batch\n",
        "            assert len(next_batch_beam) == batch_size * num_beams\n",
        "            beam_scores = beam_scores.new([x[0] for x in next_batch_beam])\n",
        "            beam_tokens = input_ids.new([x[1] for x in next_batch_beam])\n",
        "            beam_idx = input_ids.new([x[2] for x in next_batch_beam])\n",
        "\n",
        "            # re-order batch and update current length\n",
        "            input_ids = input_ids[beam_idx, :]\n",
        "            input_ids = torch.cat([input_ids, beam_tokens.unsqueeze(1)], dim=-1)\n",
        "            cur_len = cur_len + 1\n",
        "\n",
        "            # re-order internal states\n",
        "            if past is not None:\n",
        "                past = self._reorder_cache(past, beam_idx)\n",
        "\n",
        "            # extend attention_mask for new generated input if only decoder\n",
        "            if self.config.is_encoder_decoder is False:\n",
        "                attention_mask = torch.cat(\n",
        "                    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n",
        "                )\n",
        "\n",
        "        # finalize all open beam hypotheses and add to generated hypotheses\n",
        "        for batch_idx in range(batch_size):\n",
        "            if done[batch_idx]:\n",
        "                continue\n",
        "\n",
        "            # test that beam scores match previously calculated scores if not eos and batch_idx not done\n",
        "            if eos_token_id is not None and all(\n",
        "                (token_id % vocab_size).item() != eos_token_id for token_id in next_tokens[batch_idx]\n",
        "            ):\n",
        "                assert torch.all(\n",
        "                    next_scores[batch_idx, :num_beams] == beam_scores.view(batch_size, num_beams)[batch_idx]\n",
        "                ), \"If batch_idx is not done, final next scores: {} have to equal to accumulated beam_scores: {}\".format(\n",
        "                    next_scores[:, :num_beams][batch_idx],\n",
        "                    beam_scores.view(batch_size, num_beams)[batch_idx],\n",
        "                )\n",
        "\n",
        "            # need to add best num_beams hypotheses to generated hyps\n",
        "            for beam_id in range(num_beams):\n",
        "                effective_beam_id = batch_idx * num_beams + beam_id\n",
        "                final_score = beam_scores[effective_beam_id].item()\n",
        "                final_tokens = input_ids[effective_beam_id]\n",
        "                generated_hyps[batch_idx].add(final_tokens, final_score)\n",
        "\n",
        "        # depending on whether greedy generation is wanted or not define different output_batch_size and output_num_return_sequences_per_batch\n",
        "        output_batch_size = batch_size if do_sample else batch_size * num_return_sequences\n",
        "        output_num_return_sequences_per_batch = 1 if do_sample else num_return_sequences\n",
        "\n",
        "        # select the best hypotheses\n",
        "        sent_lengths = input_ids.new(output_batch_size)\n",
        "        best = []\n",
        "\n",
        "        # retrieve best hypotheses\n",
        "        for i, hypotheses in enumerate(generated_hyps):\n",
        "            sorted_hyps = sorted(hypotheses.beams, key=lambda x: x[0])\n",
        "            for j in range(output_num_return_sequences_per_batch):\n",
        "                effective_batch_idx = output_num_return_sequences_per_batch * i + j\n",
        "                best_hyp = sorted_hyps.pop()[1]\n",
        "                sent_lengths[effective_batch_idx] = len(best_hyp)\n",
        "                best.append(best_hyp)\n",
        "\n",
        "        # shorter batches are padded\n",
        "        if sent_lengths.min().item() != sent_lengths.max().item():\n",
        "            assert pad_token_id is not None, \"`Pad_token_id` has to be defined\"\n",
        "            sent_max_len = min(sent_lengths.max().item() + 1, max_length)\n",
        "            decoded = input_ids.new(output_batch_size, sent_max_len).fill_(pad_token_id)\n",
        "\n",
        "            # fill with hypothesis and eos_token_id if necessary\n",
        "            for i, hypo in enumerate(best):\n",
        "                decoded[i, : sent_lengths[i]] = hypo\n",
        "                if sent_lengths[i] < max_length:\n",
        "                    decoded[i, sent_lengths[i]] = eos_token_id\n",
        "        else:\n",
        "            # none of the hypotheses have an eos_token\n",
        "            assert (len(hypo) == max_length for hypo in best)\n",
        "            decoded = torch.stack(best).type(torch.long).to(self.device)\n",
        "\n",
        "        return decoded\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_inputs(self, prompt_text):\n",
        "        encodings_dict = self.tokenizer.batch_encode_plus([prompt_text], padding=True)\n",
        "        input_ids = torch.tensor(encodings_dict['input_ids'], dtype=torch.int64).to(self.device)\n",
        "        attention_mask = torch.tensor(encodings_dict['attention_mask'], dtype=torch.float32).to(self.device)\n",
        "        position_ids = (attention_mask.long().cumsum(-1) - 1).to(self.device)\n",
        "        position_ids.masked_fill_(position_ids < 0, 0)\n",
        "        # Empty Past State for generating first word\n",
        "        batch_size = input_ids.size(0)\n",
        "        past_shape = [2, batch_size, self.num_attention_heads, 0, self.hidden_size // self.num_attention_heads]\n",
        "        empty_past = []\n",
        "        for i in range(self.num_layer):\n",
        "            empty_past.append(torch.empty(past_shape, dtype=torch.float32).to(self.device))\n",
        "        return input_ids, attention_mask, position_ids, empty_past\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def inference_with_io_binding(self, input_ids, position_ids, attention_mask, past):\n",
        "        \"\"\"\n",
        "        For performing gpt2 inference using our persistant session.\n",
        "        \"\"\"\n",
        "        output_shapes = Gpt2Helper.get_output_shapes(batch_size=input_ids.size(0), past_sequence_length=past[0].size(3),\n",
        "            sequence_length=input_ids.size(1), config=self.config\n",
        "        )\n",
        "        output_buffers = Gpt2Helper.get_output_buffers(output_shapes, self.device, is_float16=self.is_float16)\n",
        "        io_binding = Gpt2Helper.prepare_io_binding(self.session, input_ids, position_ids, attention_mask,\n",
        "            past, output_buffers, output_shapes\n",
        "        )\n",
        "        self.session.run_with_iobinding(io_binding)\n",
        "        outputs = Gpt2Helper.get_outputs_from_io_binding_buffer(self.session,output_buffers,output_shapes,\n",
        "            return_numpy=False\n",
        "        )\n",
        "        return outputs\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def top_k_top_p_filtering(self, logits: Tensor, top_k: int = 0, top_p: float = 1.0,\n",
        "        filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1,\n",
        "        ) -> torch.Tensor:\n",
        "        if top_k > 0:\n",
        "            top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))  # Safety check\n",
        "            # Remove all tokens with a probability less than the last token of the top-k\n",
        "            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "            logits[indices_to_remove] = filter_value\n",
        "        if top_p < 1.0:\n",
        "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "            # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            if min_tokens_to_keep > 1:\n",
        "                # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
        "                sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n",
        "            # Shift the indices to the right to keep also the first token above the threshold\n",
        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "            sorted_indices_to_remove[..., 0] = 0\n",
        "            # scatter sorted tensors to original indexing\n",
        "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "            logits[indices_to_remove] = filter_value\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def postprocess_next_token_scores(self, scores, input_ids, no_repeat_ngram_size, bad_words_ids, cur_len, min_length,\n",
        "        max_length, eos_token_id, repetition_penalty, batch_size, num_beams,\n",
        "    ):\n",
        "        # repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)\n",
        "        if repetition_penalty != 1.0:\n",
        "            self.enforce_repetition_penalty_(\n",
        "                scores,\n",
        "                batch_size,\n",
        "                num_beams,\n",
        "                input_ids,\n",
        "                repetition_penalty,\n",
        "            )\n",
        "\n",
        "        # set eos token prob to zero if min_length is not reached\n",
        "        if eos_token_id is not None and cur_len < min_length:\n",
        "            scores[:, eos_token_id] = -float(\"inf\")\n",
        "\n",
        "        if no_repeat_ngram_size > 0:\n",
        "            # calculate a list of banned tokens to prevent repetitively generating the same ngrams\n",
        "            num_batch_hypotheses = batch_size * num_beams\n",
        "            # from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345\n",
        "            banned_batch_tokens = calc_banned_ngram_tokens(\n",
        "                input_ids, num_batch_hypotheses, no_repeat_ngram_size, cur_len\n",
        "            )\n",
        "            for i, banned_tokens in enumerate(banned_batch_tokens):\n",
        "                scores[i, banned_tokens] = -float(\"inf\")\n",
        "\n",
        "        if bad_words_ids is not None:\n",
        "            # Exclude EOS token (already processed)\n",
        "            bad_words_ids = list(filter(lambda bad_token_seq: bad_token_seq != [eos_token_id], bad_words_ids))\n",
        "            # calculate a list of banned tokens according to bad words\n",
        "            banned_tokens = calc_banned_bad_words_ids(input_ids.tolist(), bad_words_ids)\n",
        "            # Modify the scores in place by setting the banned tokens logits to `-inf`\n",
        "            set_scores_to_inf_for_banned_tokens(scores, banned_tokens)\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
        "        \"\"\"\n",
        "        Implement in subclasses of :class:`~transfomers.PreTrainedModel` for custom behavior to prepare inputs in the\n",
        "        generate method.\n",
        "        \"\"\"\n",
        "        return {\"input_ids\": input_ids}\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past: Tuple, beam_idx: Tensor) -> Tuple[Tensor]:\n",
        "        return tuple(layer_past.index_select(1, beam_idx) for layer_past in past)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gfeA06WgDNk",
        "colab_type": "text"
      },
      "source": [
        "# Load optimized onnx model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KSau7MEJLE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "onnx_model = GPT2ONNXModel(\n",
        "  'gpt2-final/gpt2_fp16.onnx', \n",
        "  'gpt2-xl', \n",
        "  device='cuda', \n",
        "  verbose=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b8ccyxG0q87",
        "colab_type": "text"
      },
      "source": [
        "#Generate some text..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZI69JwbJcV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "generated = onnx_model.generate(\n",
        "  'George Washington was', \n",
        "  temperature=1.0, \n",
        "  top_k=50, \n",
        "  top_p=0.92,\n",
        "  max_length=100,\n",
        "  min_length=5,\n",
        "  num_return_sequences=3,\n",
        "  repetition_penalty=1.0,\n",
        "  do_sample=True\n",
        ")\n",
        "print(generated)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}