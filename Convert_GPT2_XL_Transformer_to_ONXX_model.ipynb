{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convert GPT2-XL Transformer to ONXX model.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1Nxj0x9xXb52QKn43UKcmSZMSPplVk1Ol",
      "authorship_tag": "ABX9TyM+Hxc0bFp4iGN+ATmsJpOB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rayhern/convert-gpt2-xl-to-onnx/blob/master/Convert_GPT2_XL_Transformer_to_ONXX_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMeP6tQReZfa",
        "colab_type": "text"
      },
      "source": [
        "# Create ONNX model from transformers models. (Updated: 9-17-2020)\n",
        "- You must run this notebook in Google Colab Pro.\n",
        "- The instance needs to be of type GPU with High-RAM.\n",
        "- By: Ray Hernandez [github: @rayhern, twitter:@bizong]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2C3fXskxFMR",
        "colab_type": "text"
      },
      "source": [
        "# Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrU3lbPxtAS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBLWIJ00krza",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Install dependencies.\n",
        "!pip install onnx onnxruntime-gpu onnxruntime-tools transformers\n",
        "!pip install -U torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqTvoGcOQRmB",
        "colab_type": "text"
      },
      "source": [
        "#Convert GPT2-XL using ONNX Gpt2Helper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeDaHy3S3oSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "import os\n",
        "if os.path.exists('gpt2-final') is False:\n",
        "  print('making directory ./gpt2-final...')\n",
        "  %mkdir gpt2-final\n",
        "from onnxruntime_tools.transformers.gpt2_helper import Gpt2Helper, MyGPT2LMHeadModel\n",
        "# Can be encoded with cpu or gpu.\n",
        "device = 'cpu'\n",
        "# Use GPT2 model wrapper from onnxruntime's gpt2 helper supple.\n",
        "# conversion will not work without this wrapper.\n",
        "model = MyGPT2LMHeadModel.from_pretrained('gpt2-xl')\n",
        "onnx_model_path = \"gpt2-final/gpt2-xl.onnx\"\n",
        "print('converting model...')\n",
        "# Use GPT2Helper from onnxruntime tools to export GPT2-XL.\n",
        "Gpt2Helper.export_onnx(\n",
        "  model, \n",
        "  device, \n",
        "  onnx_model_path, \n",
        "  use_external_data_format=True, \n",
        "  verbose=True\n",
        ")\n",
        "print('finished.')\n",
        "\n",
        "#/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py:712: \n",
        "# FutureWarning: The `past` argument is deprecated and will be removed in a \n",
        "# future version, use `past_key_values` instead. FutureWarning,\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ty2FqJpxhvS",
        "colab_type": "text"
      },
      "source": [
        "#Optimize GPT2-XL model and convert to float16."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxhrCqlPHl6f",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "from onnxruntime_tools import optimizer\n",
        "from onnxruntime_tools.transformers.onnx_model_bert import BertOptimizationOptions\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM\n",
        ")\n",
        "tf_model = AutoModelForCausalLM.from_pretrained('gpt2-xl')\n",
        "print('num heads: %s. hidden size: %s.' % (tf_model.config.n_head, tf_model.config.n_embd))\n",
        "options = BertOptimizationOptions('gpt2')\n",
        "optimized_model = optimizer.optimize_model(\n",
        "    \"gpt2-final/gpt2-xl.onnx\",\n",
        "    model_type='gpt2',\n",
        "    num_heads=tf_model.config.n_head,\n",
        "    hidden_size=tf_model.config.n_embd,\n",
        "    optimization_options=options,\n",
        ")\n",
        "optimized_model.convert_model_float32_to_float16()\n",
        "optimized_model.change_input_to_int32()\n",
        "optimized_model.save_model_to_file(\"gpt2-final/gpt2_fp16.onnx\", use_external_data_format=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7849x3H6xx5",
        "colab_type": "text"
      },
      "source": [
        "#ONNX Model Text Generation Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXgFV3scD2OI",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "from psutil import cpu_count\n",
        "from os import environ\n",
        "# Constants from the performance optimization available in onnxruntime\n",
        "# It needs to be done before importing onnxruntime\n",
        "environ[\"OMP_NUM_THREADS\"] = str(cpu_count(logical=True))\n",
        "environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'\n",
        "from onnxruntime_tools.transformers.gpt2_helper import Gpt2Helper\n",
        "from onnxruntime.capi._pybind_state import set_seed as ort_set_seed\n",
        "import onnxruntime as ort\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "import numpy\n",
        "from typing import Iterable, List, Optional, Tuple\n",
        "import random\n",
        "\n",
        "@torch.no_grad()\n",
        "def calc_banned_ngram_tokens(prev_input_ids: Tensor, num_hypos: int, no_repeat_ngram_size: int, cur_len: int) -> List:\n",
        "    \"\"\"Copied from fairseq for no_repeat_ngram in beam_search\"\"\"\n",
        "    if cur_len + 1 < no_repeat_ngram_size:\n",
        "        # return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\n",
        "        return [[] for _ in range(num_hypos)]\n",
        "    generated_ngrams = [{} for _ in range(num_hypos)]\n",
        "    for idx in range(num_hypos):\n",
        "        gen_tokens = prev_input_ids[idx].tolist()\n",
        "        generated_ngram = generated_ngrams[idx]\n",
        "        for ngram in zip(*[gen_tokens[i:] for i in range(no_repeat_ngram_size)]):\n",
        "            prev_ngram_tuple = tuple(ngram[:-1])\n",
        "            generated_ngram[prev_ngram_tuple] = generated_ngram.get(prev_ngram_tuple, []) + [ngram[-1]]\n",
        "\n",
        "    def _get_generated_ngrams(hypo_idx):\n",
        "        # Before decoding the next token, prevent decoding of ngrams that have already appeared\n",
        "        start_idx = cur_len + 1 - no_repeat_ngram_size\n",
        "        ngram_idx = tuple(prev_input_ids[hypo_idx, start_idx:cur_len].tolist())\n",
        "        return generated_ngrams[hypo_idx].get(ngram_idx, [])\n",
        "\n",
        "    banned_tokens = [_get_generated_ngrams(hypo_idx) for hypo_idx in range(num_hypos)]\n",
        "    return banned_tokens\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def enforce_repetition_penalty_(lprobs, batch_size, num_beams, prev_output_tokens, repetition_penalty):\n",
        "    \"\"\"\n",
        "    Enforce the repetition penalty (from the `CTRL paper <https://arxiv.org/abs/1909.05858>`__).\n",
        "    \"\"\"\n",
        "    for i in range(batch_size * num_beams):\n",
        "        for previous_token in set(prev_output_tokens[i].tolist()):\n",
        "            # if score < 0 then repetition penalty has to multiplied to reduce the previous token probability\n",
        "            if lprobs[i, previous_token] < 0:\n",
        "                lprobs[i, previous_token] *= repetition_penalty\n",
        "            else:\n",
        "                lprobs[i, previous_token] /= repetition_penalty\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def calc_banned_bad_words_ids(prev_input_ids: Iterable[int], bad_words_ids: Iterable[int]) -> Iterable[int]:\n",
        "    banned_tokens = []\n",
        "    def _tokens_match(prev_tokens, tokens):\n",
        "        if len(tokens) == 0:\n",
        "            # if bad word tokens is just one token always ban it\n",
        "            return True\n",
        "        if len(tokens) > len(prev_tokens):\n",
        "            # if bad word tokens are longer than prev tokens they can't be equal\n",
        "            return False\n",
        "\n",
        "        if prev_tokens[-len(tokens):] == tokens:\n",
        "            # if tokens match\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    for prev_input_ids_slice in prev_input_ids:\n",
        "        banned_tokens_slice = []\n",
        "        for banned_token_seq in bad_words_ids:\n",
        "            assert len(banned_token_seq) > 0, \"Banned words token sequences {} cannot have an empty list\".format(\n",
        "                bad_words_ids\n",
        "            )\n",
        "            if _tokens_match(prev_input_ids_slice, banned_token_seq[:-1]) is False:\n",
        "                # if tokens do not match continue\n",
        "                continue\n",
        "            banned_tokens_slice.append(banned_token_seq[-1])\n",
        "        banned_tokens.append(banned_tokens_slice)\n",
        "    return banned_tokens\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def set_scores_to_inf_for_banned_tokens(scores: torch.Tensor, banned_tokens: List[List[int]]) -> None:\n",
        "    \"\"\"Modifies the scores in place by setting the banned token positions to `-inf`. Banned token is expected to be\n",
        "    a list of list of banned tokens to ban in the format [[batch index, vocabulary position],...]\n",
        "        Args:\n",
        "            scores: logits distribution of shape (batch size, vocabulary size)\n",
        "            banned_tokens: list of list of tokens to ban of length (batch_size)\n",
        "    \"\"\"\n",
        "    banned_mask_list = []\n",
        "    for idx, batch_banned_tokens in enumerate(banned_tokens):\n",
        "        for token in batch_banned_tokens:\n",
        "            banned_mask_list.append([idx, token])\n",
        "    if not banned_mask_list:\n",
        "        return\n",
        "    banned_mask = torch.LongTensor(banned_mask_list)\n",
        "    indices = torch.ones(len(banned_mask))\n",
        "    # A sparse tensor is generated from a list of coordinates: [[0, 1], [0, 2], [2, 0]].\n",
        "    banned_mask = torch.sparse.Tensor(banned_mask.t(), indices, scores.size()).to(scores.device).to_dense().bool()\n",
        "    scores.masked_fill_(banned_mask, -float(\"inf\"))\n",
        "\n",
        "class GPT2ONNXModel:\n",
        "    def __init__(self, onnx_model_path, gpt2_model_path, device='cuda', verbose=False):\n",
        "        self.config = AutoConfig.from_pretrained(gpt2_model_path)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(gpt2_model_path)\n",
        "        self.tokenizer.padding_side = \"left\"\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.num_attention_heads = self.config.n_head\n",
        "        self.hidden_size = self.config.n_embd\n",
        "        self.num_layer = self.config.n_layer\n",
        "        self.verbose = verbose\n",
        "        # Set the seed for onnxruntime.\n",
        "        torch.random.manual_seed(random.randint(1, 9999999))\n",
        "        torch.cuda.manual_seed(random.randint(1, 9999999))\n",
        "        torch.manual_seed(random.randint(1, 9999999))\n",
        "        numpy.random.seed(random.randint(1, 9999999))\n",
        "        ort_set_seed(random.randint(1, 9999999))\n",
        "        # Set our session options.\n",
        "        options = ort.SessionOptions()\n",
        "        options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
        "        # Only CPU supports parallel.\n",
        "        if device == 'cpu':\n",
        "            options.execution_mode = ort.ExecutionMode.ORT_PARALLEL\n",
        "        # Start the ONNX session.\n",
        "        self.session = ort.InferenceSession(\n",
        "            onnx_model_path,\n",
        "            sess_options=options,\n",
        "            providers=['CUDAExecutionProvider' if device == 'cuda' else 'CPUExecutionProvider']\n",
        "        )\n",
        "        self.device = device\n",
        "        if self.verbose is True:\n",
        "            print('ONNX Session Created!')\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, input_text, max_length=30, min_length=10, temperature: Optional[float] = None,\n",
        "        do_sample: Optional[bool] = True, top_k: Optional[int] = None, top_p: Optional[float] = None,\n",
        "        bad_words_ids: Optional[Iterable[int]] = None, repetition_penalty: Optional[float] = None,\n",
        "        num_return_sequences: Optional[int] = None, no_repeat_ngram_size: Optional[int] = None,\n",
        "        decoder_start_token_id: Optional[int] = None, length_penalty: Optional[float] = None,\n",
        "        ):\n",
        "        # Get the config variables from model config if not user supplied.\n",
        "        max_length = max_length if max_length is not None else self.config.max_length\n",
        "        min_length = min_length if min_length is not None else self.config.min_length\n",
        "        temperature = temperature if temperature is not None else self.config.temperature\n",
        "        top_k = top_k if top_k is not None else self.config.top_k\n",
        "        top_p = top_p if top_p is not None else self.config.top_p\n",
        "        do_sample = do_sample if do_sample is not None else self.config.do_sample\n",
        "        no_repeat_ngram_size = no_repeat_ngram_size if no_repeat_ngram_size is not None else self.config.no_repeat_ngram_size\n",
        "        num_return_sequences = num_return_sequences if num_return_sequences is not None else self.config.num_return_sequences\n",
        "        repetition_penalty = repetition_penalty if repetition_penalty is not None else self.config.repetition_penalty\n",
        "        bad_words_ids = bad_words_ids if bad_words_ids is not None else self.config.bad_words_ids\n",
        "        decoder_start_token_id = decoder_start_token_id if decoder_start_token_id is not None else self.config.decoder_start_token_id\n",
        "        length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n",
        "        if self.verbose is True:\n",
        "            print('top_k: %s. top_p: %s. temperature: %s.' % (top_k, top_p, temperature))\n",
        "        # Make sure user inputs are correct.\n",
        "        assert isinstance(max_length, int) and max_length > 0, \"`max_length` should be a strictly positive integer.\"\n",
        "        assert isinstance(min_length, int) and min_length >= 0, \"`min_length` should be a positive integer.\"\n",
        "        assert isinstance(do_sample, bool), \"`do_sample` should be a boolean.\"\n",
        "        assert temperature > 0, \"`temperature` should be strictly positive.\"\n",
        "        assert isinstance(top_k, int) and top_k >= 0, \"`top_k` should be a positive integer.\"\n",
        "        assert 0 <= top_p <= 1, \"`top_p` should be between 0 and 1.\"\n",
        "        assert repetition_penalty >= 1.0, \"`repetition_penalty` should be >= 1.\"\n",
        "        assert (\n",
        "            isinstance(no_repeat_ngram_size, int) and no_repeat_ngram_size >= 0\n",
        "        ), \"`no_repeat_ngram_size` should be a positive integer.\"\n",
        "        # Get end of string token from the tokenizer.\n",
        "        eos_token_id = self.tokenizer.eos_token_id\n",
        "        # Get all sequences requested.\n",
        "        sequences = []\n",
        "        for seq in range(num_return_sequences):\n",
        "            # Get our original input ids, attention mask, and position_ids from input_text.\n",
        "            input_ids, attention_mask, position_ids, past = self.get_inputs(input_text)\n",
        "            if self.verbose is True:\n",
        "                print(\"sequence:\", seq + 1)\n",
        "                print(\"input_ids:\", input_ids)\n",
        "                print(\"attention_mask:\", attention_mask)\n",
        "                print(\"position_ids:\", position_ids)\n",
        "            batch_size = input_ids.size(0)\n",
        "            has_eos = torch.zeros(batch_size, dtype=torch.bool).to(self.device)\n",
        "            all_token_ids = input_ids.clone()\n",
        "            # length of generated sentences / unfinished sentences\n",
        "            unfinished_sents = input_ids.new(batch_size).fill_(1)\n",
        "            sent_lengths = input_ids.new(batch_size).fill_(max_length)\n",
        "            # Get X amount of tokens/words for each sequence.\n",
        "            for step in range(max_length):\n",
        "                outputs = self.inference_with_io_binding(\n",
        "                    input_ids,\n",
        "                    position_ids,\n",
        "                    attention_mask,\n",
        "                    past\n",
        "                )\n",
        "                next_token_logits = outputs[0][:, -1, :]\n",
        "                # post process scores.\n",
        "                scores = self.postprocess_next_token_scores(\n",
        "                    next_token_logits,\n",
        "                    input_ids,\n",
        "                    no_repeat_ngram_size,\n",
        "                    bad_words_ids,\n",
        "                    step,\n",
        "                    min_length,\n",
        "                    max_length,\n",
        "                    eos_token_id,\n",
        "                    batch_size,\n",
        "                    1,\n",
        "                    repetition_penalty\n",
        "                )\n",
        "                if do_sample:\n",
        "                    # Temperature (higher temperature => more likely to sample low probability tokens)\n",
        "                    if temperature != 1.0:\n",
        "                        scores = scores / temperature\n",
        "                    # Top-p/top-k filtering\n",
        "                    next_token_logscores = self.top_k_top_p_filtering(scores, top_k, top_p)\n",
        "                    # Sample\n",
        "                    probs = F.softmax(next_token_logscores, dim=-1)\n",
        "                    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
        "                else:\n",
        "                    # Greedy decoding\n",
        "                    next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
        "                has_eos = has_eos | (next_tokens == eos_token_id)\n",
        "                tokens_to_add = next_tokens.masked_fill(has_eos, eos_token_id)\n",
        "                all_token_ids = torch.cat([all_token_ids, tokens_to_add.unsqueeze(-1)], dim=-1)\n",
        "                # Update input_ids, attention_mask, position_ids and past\n",
        "                input_ids = tokens_to_add.clone().detach().reshape([batch_size, 1])\n",
        "                position_ids = (position_ids[:, -1] + 1).reshape(batch_size, 1)\n",
        "                attention_mask = torch.cat([attention_mask, torch.ones([batch_size, 1]).type_as(attention_mask)], 1)\n",
        "                past = []\n",
        "                for i in range(self.num_layer):\n",
        "                    past_i = torch.from_numpy(outputs[i + 1]) if isinstance(outputs[i + 1], numpy.ndarray) else outputs[i + 1].clone().detach()\n",
        "                    past.append(past_i)\n",
        "\n",
        "                if torch.all(has_eos):\n",
        "                    break\n",
        "\n",
        "                if eos_token_id is not None:\n",
        "                    eos_in_sents = tokens_to_add == eos_token_id\n",
        "                    # if sentence is unfinished and the token to add is eos, sent_lengths is filled with current length\n",
        "                    is_sents_unfinished_and_token_to_add_is_eos = unfinished_sents.mul(eos_in_sents.long()).bool()\n",
        "                    sent_lengths.masked_fill_(is_sents_unfinished_and_token_to_add_is_eos, step)\n",
        "                    # unfinished_sents is set to zero if eos in sentence\n",
        "                    unfinished_sents.mul_((~eos_in_sents).long())\n",
        "\n",
        "                # stop when there is a </s> in each sentence, or if we exceed the maximul length\n",
        "                if unfinished_sents.max() == 0:\n",
        "                    break\n",
        "\n",
        "            sequences += self.tokenizer.batch_decode(all_token_ids.tolist(), skip_special_tokens=True)\n",
        "        return sequences\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_inputs(self, prompt_text):\n",
        "        encodings_dict = self.tokenizer.batch_encode_plus([prompt_text], padding=True)\n",
        "        input_ids = torch.tensor(encodings_dict['input_ids'], dtype=torch.int64).to(self.device)\n",
        "        attention_mask = torch.tensor(encodings_dict['attention_mask'], dtype=torch.float32).to(self.device)\n",
        "        position_ids = (attention_mask.long().cumsum(-1) - 1).to(self.device)\n",
        "        position_ids.masked_fill_(position_ids < 0, 0)\n",
        "        # Empty Past State for generating first word\n",
        "        batch_size = input_ids.size(0)\n",
        "        past_shape = [2, batch_size, self.num_attention_heads, 0, self.hidden_size // self.num_attention_heads]\n",
        "        empty_past = []\n",
        "        for i in range(self.num_layer):\n",
        "            empty_past.append(torch.empty(past_shape, dtype=torch.float32).to(self.device))\n",
        "        return input_ids, attention_mask, position_ids, empty_past\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def inference_with_io_binding(self, input_ids, position_ids, attention_mask, past):\n",
        "        output_shapes = Gpt2Helper.get_output_shapes(\n",
        "            batch_size=input_ids.size(0),\n",
        "            past_sequence_length=past[0].size(3),\n",
        "            sequence_length=input_ids.size(1),\n",
        "            config=self.config\n",
        "        )\n",
        "        output_buffers = Gpt2Helper.get_output_buffers(output_shapes, self.device)\n",
        "        io_binding = Gpt2Helper.prepare_io_binding(\n",
        "            self.session,\n",
        "            input_ids,\n",
        "            position_ids,\n",
        "            attention_mask,\n",
        "            past,\n",
        "            output_buffers,\n",
        "            output_shapes\n",
        "        )\n",
        "        self.session.run_with_iobinding(io_binding)\n",
        "        outputs = Gpt2Helper.get_outputs_from_io_binding_buffer(\n",
        "            self.session,\n",
        "            output_buffers,\n",
        "            output_shapes,\n",
        "            return_numpy=False\n",
        "        )\n",
        "        return outputs\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def top_k_top_p_filtering(self, logits: Tensor, top_k: int = 0, top_p: float = 1.0,\n",
        "        filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1,\n",
        "        ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            logits: logits distribution shape (batch size, vocabulary size)\n",
        "            top_k: if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p: if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "            filter_value: value to use for filtering.\n",
        "            min_tokens_to_keep: The minimum amount of tokens to keep.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor()\n",
        "\n",
        "        \"\"\"\n",
        "        if top_k > 0:\n",
        "            top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))  # Safety check\n",
        "            # Remove all tokens with a probability less than the last token of the top-k\n",
        "            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "            logits[indices_to_remove] = filter_value\n",
        "        if top_p < 1.0:\n",
        "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "            # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            if min_tokens_to_keep > 1:\n",
        "                # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
        "                sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n",
        "            # Shift the indices to the right to keep also the first token above the threshold\n",
        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "            sorted_indices_to_remove[..., 0] = 0\n",
        "            # scatter sorted tensors to original indexing\n",
        "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "            logits[indices_to_remove] = filter_value\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def postprocess_next_token_scores(self, scores, input_ids, no_repeat_ngram_size, bad_words_ids, cur_len,\n",
        "        min_length, max_length, eos_token_id, batch_size, num_beams, repetition_penalty\n",
        "        ):\n",
        "        # repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)\n",
        "        if repetition_penalty != 1.0:\n",
        "            enforce_repetition_penalty_(\n",
        "              scores,\n",
        "              batch_size,\n",
        "              num_beams,\n",
        "              input_ids,\n",
        "              repetition_penalty,\n",
        "            )\n",
        "\n",
        "        # set eos token prob to zero if min_length is not reached\n",
        "        if eos_token_id is not None and cur_len < min_length:\n",
        "            scores[:, eos_token_id] = -float(\"inf\")\n",
        "\n",
        "        if no_repeat_ngram_size > 0:\n",
        "            # calculate a list of banned tokens to prevent repetitively generating the same ngrams\n",
        "            num_batch_hypotheses = batch_size * num_beams\n",
        "            banned_batch_tokens = calc_banned_ngram_tokens(\n",
        "                input_ids, num_batch_hypotheses, no_repeat_ngram_size, cur_len\n",
        "            )\n",
        "            for i, banned_tokens in enumerate(banned_batch_tokens):\n",
        "                scores[i, banned_tokens] = -float(\"inf\")\n",
        "\n",
        "        if bad_words_ids is not None:\n",
        "            # Exclude EOS token (already processed)\n",
        "            bad_words_ids = list(filter(lambda bad_token_seq: bad_token_seq != [eos_token_id], bad_words_ids))\n",
        "            # calculate a list of banned tokens according to bad words\n",
        "            banned_tokens = calc_banned_bad_words_ids(input_ids.tolist(), bad_words_ids)\n",
        "            # Modify the scores in place by setting the banned tokens logits to `-inf`\n",
        "            set_scores_to_inf_for_banned_tokens(scores, banned_tokens)\n",
        "\n",
        "        return scores\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KSau7MEJLE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "onnx_model = GPT2ONNXModel(\n",
        "  'gpt2-final/gpt2_fp16.onnx', \n",
        "  'gpt2-xl', \n",
        "  device='cuda', \n",
        "  verbose=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b8ccyxG0q87",
        "colab_type": "text"
      },
      "source": [
        "#Generate some text with your new GPT2-XL optimized model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZI69JwbJcV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "generated = onnx_model.generate(\n",
        "  'George Washington was', \n",
        "  temperature=1.0, \n",
        "  top_k=50, \n",
        "  top_p=0.92,\n",
        "  max_length=100,\n",
        "  min_length=5,\n",
        "  num_return_sequences=3,\n",
        "  repetition_penalty=1.0,\n",
        "  do_sample=True\n",
        ")\n",
        "print(generated)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}